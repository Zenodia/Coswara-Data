{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'healthy',\n",
       " 1: 'resp_illness_not_identified',\n",
       " 2: 'no_resp_illness_exposed',\n",
       " 3: 'recovered_full',\n",
       " 4: 'positive_mild',\n",
       " 5: 'positive_asymp',\n",
       " 6: 'positive_moderate'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num2labels={0: 'healthy', 1: 'resp_illness_not_identified', 2: 'no_resp_illness_exposed', 3: 'recovered_full', 4: 'positive_mild', 5: 'positive_asymp', 6: 'positive_moderate'}\n",
    "num2labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9128409846972722, 0.08715901530272788)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "healthy_cnt=1198+77+97\n",
    "covid_pos_cnt=84+23+14+10\n",
    "tot=healthy_cnt+covid_pos_cnt\n",
    "healthy_cnt/tot, covid_pos_cnt/tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'healthy', 1: 'covid_positive'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shirnked_labels={0: 'healthy',\n",
    " 1: 'covid_positive'}\n",
    "shirnked_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 1172\n",
      "Test set size: 292\n"
     ]
    }
   ],
   "source": [
    "# dataset downloaded from https://github.com/iiscleap/Coswara-Data\n",
    "class CoughingDataset(Dataset):\n",
    "#rapper for the Cough dataset\n",
    "    # Argument List\n",
    "    #  path to the Cough csv file    \n",
    "    def __init__(self, csv_path):\n",
    "        csvData = pd.read_csv(csv_path)\n",
    "        #initialize lists to hold file names, labels, and folder numbers\n",
    "        self.labels = []\n",
    "        self.file_path = []\n",
    "        self.indexes = []\n",
    "        #loop through the csv entries and only add entries from folders in the folder list\n",
    "        for i in range(0,len(csvData)):\n",
    "            self.file_path.append(csvData.iloc[i, -2])\n",
    "            self.labels.append(self.__shirnk_labels__(csvData.iloc[i, 2])) \n",
    "            self.indexes.append(i)   \n",
    "    def __shirnk_labels__(self,cur_label):\n",
    "        if cur_label in [0,1,2]:\n",
    "            cur_label=0\n",
    "        elif cur_label in [3,4,5,6]:\n",
    "            cur_label=1           \n",
    "        else:\n",
    "            print(\"the allowed label is 0:healthy ,1:covid_positive, but got = \", cur_label)\n",
    "        return cur_label\n",
    "    def __getitem__(self, index):\n",
    "        path = self.file_path[index]\n",
    "        #sound = torchaudio.load(path, out = None, normalization = True)\n",
    "        sound = torchaudio.load(path, out = None, normalization = True)\n",
    "        soundData = sound[0]\n",
    "        #load returns a tensor with the sound data and the sampling frequency (44.1kHz)\n",
    "        #downsample the audio to ~8kHz\n",
    "        tempData = torch.zeros([160000]) #tempData accounts for audio clips that are too short\n",
    "        if soundData.numel() < 160000:\n",
    "            tempData[:soundData.numel()] = soundData[0,:]\n",
    "        else:\n",
    "            tempData[:] = soundData[0,:160000]\n",
    "        \n",
    "        soundData = tempData\n",
    "        soundFormatted = torch.zeros([32000])\n",
    "        soundFormatted[:32000] = soundData[::5] # take every fifth sample of soundData\n",
    "        soundFormatted=torch.unsqueeze(soundFormatted,0)\n",
    "        return soundFormatted , self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_path)\n",
    "\n",
    "    \n",
    "csv_path = 'train.csv'\n",
    "test_csv = 'test.csv'\n",
    "\n",
    "train_set = CoughingDataset(csv_path)\n",
    "test_set = CoughingDataset(test_csv)\n",
    "print(\"Train set size: \" + str(len(train_set)))\n",
    "print(\"Test set size: \" + str(len(test_set)))\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 128, shuffle = True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 128, shuffle = True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1, 32000]), torch.Size([128]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b=iter(test_loader).next()\n",
    "a.size(),b.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coughing model building "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we will use a convolutional neural network to process the raw audio data. Usually more advanced transforms are applied to the audio data, however CNNs can be used to accurately process the raw data. The specific architecture is modeled after the M5 network architecture described in https://arxiv.org/pdf/1610.00087.pdf. An important aspect of models processing raw audio data is the receptive field of their first layer’s filters. Our model’s first filter is length 80 so when processing audio sampled at 8kHz the receptive field is around 10ms. This size is similar to speech processing applications that often use receptive fields ranging from 20ms to 40ms. model adapted from [ here](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/audio_classifier_tutorial.ipynb#scrollTo=zFcvJkVk-jhQ)\n",
    "note: modified optimizer with NVIDIA **novogad**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID(\n",
      "  (conv1): Conv1d(1, 128, kernel_size=(80,), stride=(4,))\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (avgPool): AvgPool1d(kernel_size=(30,), stride=(30,), padding=(0,))\n",
      "  (fc1): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "class COVID(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(COVID, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 128, 80, 4)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(128, 128, 3)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(128, 256, 3)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(256, 512, 3)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.avgPool = nn.AvgPool1d(30) #input should be 512x30 so this outputs a 512x1\n",
    "        self.fc1 = nn.Linear(512, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = self.avgPool(x)\n",
    "        x = x.permute(0, 2, 1) #change the 512x1 to 1x512\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim = 2)\n",
    "\n",
    "model = COVID()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from novograd import Novograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Novograd(model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        data = data.requires_grad_() #set requires_grad to True for training\n",
    "        output = model(data)\n",
    "        output = output.permute(1, 0, 2) #original output dimensions are batchSizex1x10 \n",
    "        loss = F.nll_loss(output[0], target) #the loss functions expects a batchSizex10 input\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0: #print training stats\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        pred = output.max(2)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target).cpu().sum().item()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First round of training complete. Setting learn rate to 0.001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1172 (0%)]\tLoss: 1.143303\n",
      "Train Epoch: 2 [0/1172 (0%)]\tLoss: 0.383003\n",
      "Train Epoch: 3 [0/1172 (0%)]\tLoss: 0.222000\n",
      "Train Epoch: 4 [0/1172 (0%)]\tLoss: 0.252832\n"
     ]
    }
   ],
   "source": [
    "log_interval = 20\n",
    "epochs = 5 \n",
    "for epoch in range(1, epochs):\n",
    "    if epoch == 1:\n",
    "        print(\"First round of training complete. Setting learn rate to 0.001.\")\n",
    "    \n",
    "    elif epoch == 5:\n",
    "        test(model, epoch)\n",
    "    else:\n",
    "        pass\n",
    "    scheduler.step()\n",
    "    train(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 263/292 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='./saved_model/relabelled_convid.pt'\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='./saved_model/relabelled_convid.pt'\n",
    "loaded_model = COVID()\n",
    "loaded_model.to(device)\n",
    "opt = Novograd(loaded_model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "sch = optim.lr_scheduler.StepLR(opt, step_size = 20, gamma = 0.1)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shirnked_labels), shirnked_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset downloaded from https://github.com/iiscleap/Coswara-Data\n",
    "class On_the_fly_Coughing(Dataset):\n",
    "#rapper for the Cough dataset\n",
    "    # Argument List\n",
    "    #  path to the Cough csv file    \n",
    "    def __init__(self, csv_path , label_to_retrieve):\n",
    "        csvData = pd.read_csv(csv_path)\n",
    "        #initialize lists to hold file names, labels, and folder numbers\n",
    "        self.labels = []\n",
    "        self.file_path = []\n",
    "        self.indexes = []\n",
    "        #loop through the csv entries and only add entries from folders in the folder list\n",
    "        for i in range(0,len(csvData)):\n",
    "            if csvData.iloc[i,2] in label_to_retrieve:\n",
    "                self.file_path.append(csvData.iloc[i, -2])\n",
    "                self.labels.append(self.__shirnk_labels__(csvData.iloc[i, 2]))\n",
    "                self.indexes.append(i)   \n",
    "    def __shirnk_labels__(self,cur_label):\n",
    "        if cur_label in [0,1,2]:\n",
    "            cur_label=0\n",
    "        elif cur_label in [3,4,5,6]:\n",
    "            cur_label=1           \n",
    "        else:\n",
    "            print(\"the allowed label is 0:healthy ,1:covid_positive, but got = \", cur_label)\n",
    "        return cur_label\n",
    "    def __getitem__(self, index):\n",
    "        path = self.file_path[index]\n",
    "        print(\"wav file location \" , path)\n",
    "        print(\"label \", shirnked_labels[self.labels[index]])\n",
    "        #sound = torchaudio.load(path, out = None, normalization = True)\n",
    "        sound = torchaudio.load(path, out = None, normalization = True)\n",
    "        soundData = sound[0]\n",
    "        #load returns a tensor with the sound data and the sampling frequency (44.1kHz for UrbanSound8K)\n",
    "        #downsample the audio to ~8kHz\n",
    "        tempData = torch.zeros([160000]) #tempData accounts for audio clips that are too short\n",
    "        if soundData.numel() < 160000:\n",
    "            tempData[:soundData.numel()] = soundData[0,:]\n",
    "        else:\n",
    "            tempData[:] = soundData[0,:160000]\n",
    "        \n",
    "        soundData = tempData\n",
    "        soundFormatted = torch.zeros([32000])\n",
    "        soundFormatted[:32000] = soundData[::5] #take every fifth sample of soundData\n",
    "        soundFormatted=torch.unsqueeze(soundFormatted,0)\n",
    "        return path, soundFormatted , self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_path)\n",
    "\n",
    "    \n",
    "csv_path = 'test.csv'\n",
    "\n",
    "positive_with_covid=On_the_fly_Coughing(csv_path,[3,4,6] )\n",
    "healthy=On_the_fly_Coughing(csv_path,[0])\n",
    "asymptomatic = On_the_fly_Coughing(csv_path,[5])\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n",
    "asymptomatic_loader = torch.utils.data.DataLoader(asymptomatic, batch_size = 1, shuffle = True, **kwargs)\n",
    "covid_excl_asy_loader = torch.utils.data.DataLoader(positive_with_covid, batch_size = 1, shuffle = True, **kwargs)\n",
    "healthy_loader = torch.utils.data.DataLoader(healthy, batch_size = 1, shuffle = True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyn_wav_loc, asyn_in_data, asyn_lb= iter(asymptomatic_loader).next()\n",
    "covid_pos_wav_loc, covid_pos_in_data, covid_lb= iter(covid_excl_asy_loader).next()\n",
    "healthy_wav_loc, healthy_pos_in_data, healthy_lb= iter(healthy_loader).next()\n",
    "\n",
    "print(asyn_wav_loc[0], asyn_lb)\n",
    "print(covid_pos_wav_loc[0], covid_lb)\n",
    "print(healthy_wav_loc[0], healthy_lb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "def listen2audio(wav_loc, lb):\n",
    "    audio_sample=wav_loc[0]\n",
    "    print(\"originally labelled by expert as {}\".format(shirnked_labels[lb.item()])  )\n",
    "    return ipd.Audio(audio_sample) # load a local WAV file\n",
    "\n",
    "listen2audio(covid_pos_wav_loc, covid_lb)\n",
    "#listen2audio(healthy_wav_loc, healthy_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from librosa.display import waveplot\n",
    "import IPython.display as ipd\n",
    "%matplotlib inline\n",
    "def visualize_waveform(wav_loc, lb):\n",
    "    # Load and listen to the audio file\n",
    "    example_file = wav_loc[0]\n",
    "    audio, sample_rate = librosa.load(example_file)\n",
    "\n",
    "    #ipd.Audio(example_file, rate=sample_rate)\n",
    "    # Plot our example audio file's waveform\n",
    "    plt.rcParams['figure.figsize'] = (15,7)\n",
    "    plt.title('Waveform of **{}** Example'.format(shirnked_labels[lb.item()]))\n",
    "    plt.ylabel('Amplitude')\n",
    "\n",
    "    waveplot(audio)\n",
    "    plt.show()\n",
    "    return audio\n",
    "\n",
    "\n",
    "asyn_audio=visualize_waveform(asyn_wav_loc, asyn_lb)\n",
    "covid_audio=visualize_waveform(covid_pos_wav_loc, covid_lb)\n",
    "healthy_audio=visualize_waveform(healthy_wav_loc, healthy_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def visualize_spectrogram(audio, lb):\n",
    "    # Get spectrogram using Librosa's Short-Time Fourier Transform (stft)\n",
    "    spec = np.abs(librosa.stft(audio))\n",
    "    spec_db = librosa.amplitude_to_db(spec, ref=np.max)  # Decibels\n",
    "\n",
    "    # Use log scale to view frequencies\n",
    "    librosa.display.specshow(spec_db, y_axis='log', x_axis='time')\n",
    "    plt.colorbar()\n",
    "    plt.title('Audio Spectrogram **{}** Example'.format(shirnked_labels[lb.item()]))\n",
    "    plt.show()\n",
    "visualize_spectrogram(asyn_audio, asyn_lb)\n",
    "visualize_spectrogram(covid_audio, covid_lb)\n",
    "visualize_spectrogram(healthy_audio, healthy_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_on_the_fly_prediction(input_data, loaded_model,lb):\n",
    "    out= loaded_model(input_data)\n",
    "    out = out.permute(1, 0, 2)\n",
    "    pred = out.max(2)[1].item()\n",
    "    assert type(pred)==int\n",
    "    return shirnked_labels[pred]\n",
    "pred=get_on_the_fly_prediction(healthy_pos_in_data.cuda(), loaded_model,healthy_lb )\n",
    "print(\"prediction\", pred, \"| true label \", shirnked_labels[healthy_lb.item()])\n",
    "pred=get_on_the_fly_prediction(asyn_in_data.cuda(), loaded_model, asyn_lb )\n",
    "print(\"prediction\", pred , \"| true label \", shirnked_labels[asyn_lb.item()])\n",
    "pred=get_on_the_fly_prediction(covid_pos_in_data.cuda(),loaded_model, covid_lb)\n",
    "print(\"prediction\", pred, \"| true label \" , shirnked_labels[covid_lb.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "input_names = [ \"input\" ] + [ \"learned_%d\" % i for i in range(30) ]\n",
    "output_names = [ \"output\" ]\n",
    "\n",
    "torch.onnx.export(model, asyn_in_data.cuda(), \"./saved_model/relabelled_covid.onnx\", verbose=True, input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
